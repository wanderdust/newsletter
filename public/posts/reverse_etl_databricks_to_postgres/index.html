<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming | Pablo Lopez</title>
<meta name=keywords content="postgres,databricks"><meta name=description content="In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.
The goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time."><meta name=author content><link rel=canonical href=http://localhost:1313/posts/reverse_etl_databricks_to_postgres/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/reverse_etl_databricks_to_postgres/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/reverse_etl_databricks_to_postgres/"><meta property="og:site_name" content="Pablo Lopez"><meta property="og:title" content="Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming"><meta property="og:description" content="In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.
The goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-08T13:21:30+01:00"><meta property="article:modified_time" content="2025-05-08T13:21:30+01:00"><meta property="article:tag" content="Postgres"><meta property="article:tag" content="Databricks"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming"><meta name=twitter:description content="In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.
The goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming","item":"http://localhost:1313/posts/reverse_etl_databricks_to_postgres/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming","name":"Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming","description":"In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.\nThe goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time.\n","keywords":["postgres","databricks"],"articleBody":"In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.\nThe goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time.\nData warehouses are optimised for analytics and they won’t perform well if you want low latency over a high volume of requests. On the other hand, Postgres leverages indexes to make reading data very fast.\nThis blog post shows a step by step implementation of a Reverse ETL pipeline using Delta’s structured streaming. I’ll begin by defining the requirements, then demonstrate how to implement them using Spark. Next, I’ll cover how to orchestrate the pipelines, and finally, I’ll show how to optimise the tables in Postgres for fast reads.\nLet’s dive in!\nThe Jargon Let’s get the jargon out of the way first. Reverse ETL is an overcomplicated name that refers to moving data from the data warehouse to an external system. That’s it!\nWhat are our requirements? Before we start implementing, there are some things we need to find out: how often does the data update? how fresh does the data need to be on the consumer end? What’s the data volume? Is the data append only or are there upserts and deletes?\nThe answer to these questions will shape the data pipeline design to satisfy the end-users’ needs.\nFor our use case let’s imagine we have the following requirements:\nThe source Delta table is 50 million rows, approximately 8GB in size. The load type is primarily updates and some appends, no deletes. The source data is updated every 30 minutes. The end users want to have this data available within 5 minutes of the data being updated. Rows updated per refresh is around 40k rows. Design the pipeline We can start shaping a good design to meet those requirements. The data is updated every 30 minutes, so a batch job will do. No need to burn money on a continuous streaming job.\nInterestingly, in Delta you can create streaming jobs that run on a schedule by using the availableNow trigger. This trigger processes all available data up to that point. The cool thing about it is that you can easily convert it into a streaming job later on should your needs change by simply removing the trigger.\nOnly 40k rows out of 50 million are updated each time (0.08% of the data). This makes a full reload highly inefficient. A better approach is to move only the data that has changed since the last run. We can achieve this using Delta’s Change Data Feed (CDF) which tracks row-level changes between table versions.\nFinally, we need to consider the types of updates. For our use case, the data is updated in place or appended, so we can use a MERGE query to combine both operations in a single query.\nIf you’ve not seen a MERGE query before, my ex-colleague Max cleverly defined it as “a switch statement for SQL”. Using a MERGE query is also a great way to ensure idempotency in the pipeline, ensuring there are no duplicates are introduced when you re-run your pipelines after a failed job.\nImplementation Now to the juicy bit. To start using Change Data Feed (CDF) to track row-level changes we need to enable it in the source table.\nALTER TABLE public.mycatalog.mytable SET TBLPROPERTIES (delta.enableChangeDataFeed = true) Once enabled, we can view the CDF changes in the Delta table by running the following query. The number indicates the start version you want to start querying from.\nSELECT * FROM table_changes('`public`.`mycatalog`.`mytable`', 1); The CDF output will show your data with three additional metadata columns: the change type for that row, the version when the change happened and the timestamp. It will only show the rows that have changed from the specified starting version.\nThe next step is defining the function to upsert data into Postgres. We’ll create a function that takes in two parameters: a dataframe representing the micro-batch, and a batch id.\ndef writeToPostgres(df, batch_id): with psycopg.connect(...) as conn: with conn.cursor() as cur: query = \"\"\" MERGE INTO public.mytable AS target USING ( VALUES (%s, %s, %s) , (%s, %s, %s), ... ) AS source (\"user_id\", \"name\", \"age\") ON target.user_id = source.user_id WHEN MATCHED AND source._commit_timestamp \u003e= target._commit_timestamp THEN UPDATE SET \"name\" = source.\"name\", \"age\" = source.\"age\" WHEN NOT MATCHED THEN INSERT (\"user_id\", \"name\", \"updated_at\") VALUES (source.\"user_id\", source.\"name\", source.\"age\"); \"\"\" values = (...) # dynamically generated from df cur.execute(query, values) conn.commit() Within this function we define the MERGE query which checks if a record exists or not based on a primary key (user_id). If the record exists, it uses the _commit_timespamp column to ensure only newer records are inserted. The code shows a simplified version of the MERGE query for illustrative purposes. You can build the values and columns dynamically based on your dataframe.\nBefore we plug this function into a streaming pipeline we need to deal with the duplication problem that happens when running CDF workflows on a schedule.\nAs we saw earlier, CDF returns all updated rows since the last run. If the pipeline is paused or fails to run, multiple updates to the same row may accumulate. CDF will return all these rows with its corresponding version.\nTo solve the duplication issue, we can create a function that filters the data to keep only the rows with the latest version. This will be the row with the most up to date values. Here is a function that does this:\ndef get_latest_cdf_record(df, primary_key_name): window_spec = Window.partitionBy(primary_key_name).orderBy( col(\"_commit_version\").desc(), col(\"_commit_timestamp\").desc() ) df_dedup = ( df.filter(\"_change_type != 'update_preimage'\") .withColumn(\"row_num\", row_number().over(window_spec)) .filter(col(\"row_num\") == 1) .drop(\"row_num\") ) return df_dedup This logic ensures we only keep the latest change for each primary key based on _commit_version and _commit_timestamp. The function can be applied to each batch before we upsert the data.\nprimary_key_name = 'user_id' def writeToPostgres(df: DataFrame, batch_id): df = get_latest_cdf_record(df, primary_key_name) # MERGE query logic below ... We now have all the ingredients to put this all together into a structured streaming job:\n( spark.readStream.format(\"delta\") .option(\"readChangeFeed\", \"true\") .table(\"public.mycatalog.mytable\") .writeStream.foreachBatch(writeToPostgres) .option(\"checkpointLocation\", \"/mnt/checkpoints/mycheckpoint\") .trigger(availableNow=True) .start() ) This Spark streaming pipeline does the following:\nReads from a Delta table as a stream. Enables readChangeFeed to capture only row-level changes (inserts, updates, deletes). Uses the availableNow trigger, meaning it only processes new data when executed, then stops once done. We use this trigger to simulate a batch job when using a stream. Writes each micro-batch using the writeToPostgres function. Tracks progress with a checkpoint, so it can pick up where it left off last time. Before you can run this, ensure you’ve created the tables and schema where the data is going to land on the Postgres end.\nWith this setup, we now have a fully functioning pipeline ready for testing. Copying 50k rows to Postgres takes around 30 seconds. However, the initial load of 50 million rows will take around 20 hours with the current implementation. These numbers will of course vary depending on the compute used.\nDealing with the first load The bottleneck is the first load. We are using a MERGE query with a custom psycopg connection which isn’t optimised to be fast. To speed things up, we can use the JDBC connector to write in append mode, which will be significantly faster. We can write a separate job which will run for the initial load only.\ndef writeToPostgresAppend(df, batch_id): url = \"jdbc:postgresql://localhost:5432/mydatabase\" properties = {\"user\": \"user\", \"password\": \"password\", \"driver\": \"org.postgresql.Driver\"} df.write.format('delta').jdbc(url=url, table=\"mytable\", mode=\"append\", properties=properties).save() This approach takes 40 minutes to load all 50 million rows into Postgres, which is a significant improvement. This approach has its own challenges though, for example running it more than once will create duplicates in the destination table.\nTo keep the pipeline idempotent (no duplicates if we run it multiple times), we can load the data into a temporary table in Postgres. We then swap it with the production table inside a single transaction. Swapping avoids locking or partial state in case of failure, ensuring zero-downtime for consumers.\nWe can write this logic in the job itself or define it externally as separate steps to run before and after the job. Here’s what it looks like:\ndef writeToPostgresAppend(df, batch_id): url = \"jdbc:postgresql://localhost:5432/mydatabase\" properties = {\"user\": \"user\", \"password\": \"password\", \"driver\": \"org.postgresql.Driver\"} df.write.jdbc(url=url, table=\"myschema.mytable_tmp\", mode=\"append\", properties=properties) # Create tmp table with psycopg.connect(...) as conn: with conn.transaction(): cur = conn.cursor() cur.execute(\"DROP TABLE IF EXISTS myschema.mytable_tmp;\") cur.execute(\"CREATE TABLE myschema.mytable_tmp (LIKE myschema.mytable INCLUDING ALL);\") # Run full load stream = ( spark.readStream.format(\"delta\") .option(\"checkpointLocation\", \"/mnt/checkpoints/mycheckpoint\") .table(\"public.mycatalog.mytable\") .trigger(availableNow=True) .stream.writeStream.foreachBatch(writeToPostgresAppend) .start() ).awaitTermination() # Swap tables with psycopg.connect(...) as conn: with conn.transaction(): cur = conn.cursor() cur.execute(\"ALTER TABLE myschema.mytable RENAME TO mytable_old;\") cur.execute(\"ALTER TABLE myschema.mytable_tmp RENAME TO mytable;\") cur.execute(\"DROP TABLE myschema.mytable_old;\") Now we can run this script to load all the data for the first time. If we run this multiple times it will safely swap tables without creating any downtime for our end-users consuming the data.\nNote that we are not using Change Data Feed here because we want to move all the data across not the row-level changes. It is also important that you use a fresh checkpoint when you run this so that it loads all the data and not just the appends since the last execution.\nPutting it all together The final step to put these pipeline into production is the orchestration part. In an ideal world we would start the job when the source table updates. But for simplicity’s sake we can run it on a 5 minute schedule: if there’s been an update it will process it, if not it will quickly shut down.\nTo smoothly coordinate the transition from the initial load (using append) to ongoing upserts, the simplest approach is to define two separate jobs:\nOne for the initial load, which we’ll run manually once. One for the upserts, which we’ll scheduled to run regularly thereafter. It’s important that both jobs share the same checkpoint location so the upsert job can resume from where the initial load left off.\nAs for orchestrating tools, we can use the scheduling functionality from Databricks or an external tool like Airflow.\nServing the data We now have the data landing in Postgres on a regular basis, so the last thing to do is to add indexes on the columns to improve reading speed. To know which indexes to create you need to look at your users’ query patterns.\nLet’s say for example that most of the queries filter by either user_id or age. We could create the following indexes:\nCREATE INDEX idx_user_id_btree ON mytable (user_id); CREATE INDEX idx_age_btree ON mytable (age); Conclusion We’ve built a Reverse ETL pipeline that upserts data from Delta into Postgres using Structured Streaming. It runs every 30 minutes, handles updates with Change Data Feed, and stays idempotent to avoid duplication.\nBy combining batch-style streaming with deduplication and merge logic, we’ve made warehouse data available in Postgres. This data can now be read by downstream systems with sub-second latency.\nThank you for making it this far! I hope you’ve found the post interesting. Feel free to reach out to me on Linkedin or Substack!\nThanks to Oleksandr for providing feedback on earlier versions of this post.\nMy Newsletter I send out an email every so often about cool stuff I’m building or working on. ","wordCount":"1919","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-05-08T13:21:30+01:00","dateModified":"2025-05-08T13:21:30+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/reverse_etl_databricks_to_postgres/"},"publisher":{"@type":"Organization","name":"Pablo Lopez","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Pablo Lopez (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Pablo Lopez</a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming</h1><div class=post-meta><span title='2025-05-08 13:21:30 +0100 BST'>May 8, 2025</span>&nbsp;·&nbsp;10 min</div></header><div class=post-content><p>In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.</p><p>The goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time.</p><p>Data warehouses are optimised for analytics and they won’t perform well if you want low latency over a high volume of requests. On the other hand, Postgres leverages indexes to make reading data very fast.</p><p>This blog post shows a step by step implementation of a Reverse ETL pipeline using Delta’s structured streaming. I’ll begin by defining the requirements, then demonstrate how to implement them using Spark. Next, I’ll cover how to orchestrate the pipelines, and finally, I’ll show how to optimise the tables in Postgres for fast reads.</p><p>Let’s dive in!</p><h2 id=the-jargon>The Jargon<a hidden class=anchor aria-hidden=true href=#the-jargon>#</a></h2><p>Let’s get the jargon out of the way first. Reverse ETL is an overcomplicated name that refers to moving data from the data warehouse to an external system. That’s it!</p><h2 id=what-are-our-requirements>What are our requirements?<a hidden class=anchor aria-hidden=true href=#what-are-our-requirements>#</a></h2><p>Before we start implementing, there are some things we need to find out: how often does the data update? how fresh does the data need to be on the consumer end? What’s the data volume? Is the data append only or are there upserts and deletes?</p><p>The answer to these questions will shape the data pipeline design to satisfy the end-users’ needs.</p><p>For our use case let’s imagine we have the following requirements:</p><ul><li>The source Delta table is 50 million rows, approximately 8GB in size.</li><li>The load type is primarily updates and some appends, no deletes.</li><li>The source data is updated every 30 minutes.</li><li>The end users want to have this data available within 5 minutes of the data being updated.</li><li>Rows updated per refresh is around 40k rows.</li></ul><h2 id=design-the-pipeline>Design the pipeline<a hidden class=anchor aria-hidden=true href=#design-the-pipeline>#</a></h2><p><img loading=lazy src=/posts/reverse_etl_databricks_to_postgres/diagram.png></p><p>We can start shaping a good design to meet those requirements. The data is updated every 30 minutes, so a batch job will do. No need to burn money on a continuous streaming job.</p><p>Interestingly, in Delta you can create streaming jobs that run on a schedule by using the availableNow trigger. This trigger processes all available data up to that point. The cool thing about it is that you can easily convert it into a streaming job later on should your needs change by simply removing the trigger.</p><p>Only 40k rows out of 50 million are updated each time (0.08% of the data). This makes a full reload highly inefficient. A better approach is to move only the data that has changed since the last run. We can achieve this using Delta’s Change Data Feed (CDF) which tracks row-level changes between table versions.</p><p>Finally, we need to consider the types of updates. For our use case, the data is updated in place or appended, so we can use a MERGE query to combine both operations in a single query.</p><p>If you’ve not seen a MERGE query before, my ex-colleague Max cleverly defined it as “a switch statement for SQL”. Using a MERGE query is also a great way to ensure idempotency in the pipeline, ensuring there are no duplicates are introduced when you re-run your pipelines after a failed job.</p><h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><p>Now to the juicy bit. To start using Change Data Feed (CDF) to track row-level changes we need to enable it in the source table.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>ALTER</span><span class=w> </span><span class=k>TABLE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=k>public</span><span class=p>.</span><span class=n>mycatalog</span><span class=p>.</span><span class=n>mytable</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>SET</span><span class=w> </span><span class=n>TBLPROPERTIES</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=p>(</span><span class=n>delta</span><span class=p>.</span><span class=n>enableChangeDataFeed</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>true</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><p>Once enabled, we can view the CDF changes in the Delta table by running the following query. The number indicates the start version you want to start querying from.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>table_changes</span><span class=p>(</span><span class=s1>&#39;`public`.`mycatalog`.`mytable`&#39;</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><p>The CDF output will show your data with three additional metadata columns: the change type for that row, the version when the change happened and the timestamp. It will only show the rows that have changed from the specified starting version.</p><p><img loading=lazy src=/posts/reverse_etl_databricks_to_postgres/cdc_1.png></p><p>The next step is defining the function to upsert data into Postgres. We’ll create a function that takes in two parameters: a dataframe representing the micro-batch, and a batch id.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>writeToPostgres</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>batch_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>psycopg</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=o>...</span><span class=p>)</span> <span class=k>as</span> <span class=n>conn</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>conn</span><span class=o>.</span><span class=n>cursor</span><span class=p>()</span> <span class=k>as</span> <span class=n>cur</span><span class=p>:</span> 
</span></span><span class=line><span class=cl>         
</span></span><span class=line><span class=cl>          <span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>          MERGE INTO public.mytable AS target
</span></span></span><span class=line><span class=cl><span class=s2>          USING (
</span></span></span><span class=line><span class=cl><span class=s2>            VALUES (</span><span class=si>%s</span><span class=s2>, </span><span class=si>%s</span><span class=s2>, </span><span class=si>%s</span><span class=s2>) , (</span><span class=si>%s</span><span class=s2>, </span><span class=si>%s</span><span class=s2>, </span><span class=si>%s</span><span class=s2>), ...
</span></span></span><span class=line><span class=cl><span class=s2>          ) AS source (&#34;user_id&#34;, &#34;name&#34;, &#34;age&#34;)
</span></span></span><span class=line><span class=cl><span class=s2>          ON target.user_id = source.user_id
</span></span></span><span class=line><span class=cl><span class=s2>          WHEN MATCHED AND source._commit_timestamp &gt;= target._commit_timestamp THEN
</span></span></span><span class=line><span class=cl><span class=s2>            UPDATE SET &#34;name&#34; = source.&#34;name&#34;, &#34;age&#34; = source.&#34;age&#34;
</span></span></span><span class=line><span class=cl><span class=s2>          WHEN NOT MATCHED THEN
</span></span></span><span class=line><span class=cl><span class=s2>            INSERT (&#34;user_id&#34;, &#34;name&#34;, &#34;updated_at&#34;)
</span></span></span><span class=line><span class=cl><span class=s2>            VALUES (source.&#34;user_id&#34;, source.&#34;name&#34;, source.&#34;age&#34;);
</span></span></span><span class=line><span class=cl><span class=s2>          &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>          <span class=n>values</span> <span class=o>=</span> <span class=p>(</span><span class=o>...</span><span class=p>)</span>  <span class=c1># dynamically generated from df</span>
</span></span><span class=line><span class=cl>          <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>         <span class=n>conn</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span></span></code></pre></div><p>Within this function we define the MERGE query which checks if a record exists or not based on a primary key (user_id). If the record exists, it uses the <code>_commit_timespamp</code> column to ensure only newer records are inserted. The code shows a simplified version of the MERGE query for illustrative purposes. You can build the values and columns dynamically based on your dataframe.</p><p>Before we plug this function into a streaming pipeline we need to deal with the duplication problem that happens when running CDF workflows on a schedule.</p><p>As we saw earlier, CDF returns all updated rows since the last run. If the pipeline is paused or fails to run, multiple updates to the same row may accumulate. CDF will return all these rows with its corresponding version.</p><p><img loading=lazy src=/posts/reverse_etl_databricks_to_postgres/cdc_2.png></p><p>To solve the duplication issue, we can create a function that filters the data to keep only the rows with the latest version. This will be the row with the most up to date values. Here is a function that does this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_latest_cdf_record</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>primary_key_name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>window_spec</span> <span class=o>=</span> <span class=n>Window</span><span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=n>primary_key_name</span><span class=p>)</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>col</span><span class=p>(</span><span class=s2>&#34;_commit_version&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>(),</span> <span class=n>col</span><span class=p>(</span><span class=s2>&#34;_commit_timestamp&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>df_dedup</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=s2>&#34;_change_type != &#39;update_preimage&#39;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;row_num&#34;</span><span class=p>,</span> <span class=n>row_number</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>window_spec</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;row_num&#34;</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&#34;row_num&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>df_dedup</span>
</span></span></code></pre></div><p>This logic ensures we only keep the latest change for each primary key based on <code>_commit_version</code> and <code>_commit_timestamp</code>. The function can be applied to each batch before we upsert the data.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>primary_key_name</span> <span class=o>=</span> <span class=s1>&#39;user_id&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>writeToPostgres</span><span class=p>(</span><span class=n>df</span><span class=p>:</span> <span class=n>DataFrame</span><span class=p>,</span> <span class=n>batch_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>df</span> <span class=o>=</span> <span class=n>get_latest_cdf_record</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>primary_key_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># MERGE query logic below</span>
</span></span><span class=line><span class=cl>  <span class=o>...</span>
</span></span></code></pre></div><p>We now have all the ingredients to put this all together into a structured streaming job:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;delta&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;readChangeFeed&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>table</span><span class=p>(</span><span class=s2>&#34;public.mycatalog.mytable&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>writeStream</span><span class=o>.</span><span class=n>foreachBatch</span><span class=p>(</span><span class=n>writeToPostgres</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;checkpointLocation&#34;</span><span class=p>,</span> <span class=s2>&#34;/mnt/checkpoints/mycheckpoint&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>trigger</span><span class=p>(</span><span class=n>availableNow</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>This Spark streaming pipeline does the following:</p><ul><li>Reads from a Delta table as a stream.</li><li>Enables readChangeFeed to capture only row-level changes (inserts, updates, deletes).</li><li>Uses the availableNow trigger, meaning it only processes new data when executed, then stops once done. We use this trigger to simulate a batch job when using a stream.</li><li>Writes each micro-batch using the writeToPostgres function.</li><li>Tracks progress with a checkpoint, so it can pick up where it left off last time.</li></ul><p>Before you can run this, ensure you’ve created the tables and schema where the data is going to land on the Postgres end.</p><p>With this setup, we now have a fully functioning pipeline ready for testing. Copying 50k rows to Postgres takes around 30 seconds. However, the initial load of 50 million rows will take around 20 hours with the current implementation. These numbers will of course vary depending on the compute used.</p><h2 id=dealing-with-the-first-load>Dealing with the first load<a hidden class=anchor aria-hidden=true href=#dealing-with-the-first-load>#</a></h2><p>The bottleneck is the first load. We are using a MERGE query with a custom psycopg connection which isn’t optimised to be fast. To speed things up, we can use the JDBC connector to write in append mode, which will be significantly faster. We can write a separate job which will run for the initial load only.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>writeToPostgresAppend</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>batch_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;jdbc:postgresql://localhost:5432/mydatabase&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>properties</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;user&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;password&#34;</span><span class=p>:</span> <span class=s2>&#34;password&#34;</span><span class=p>,</span> <span class=s2>&#34;driver&#34;</span><span class=p>:</span> <span class=s2>&#34;org.postgresql.Driver&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s1>&#39;delta&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>jdbc</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>url</span><span class=p>,</span> <span class=n>table</span><span class=o>=</span><span class=s2>&#34;mytable&#34;</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;append&#34;</span><span class=p>,</span> <span class=n>properties</span><span class=o>=</span><span class=n>properties</span><span class=p>)</span><span class=o>.</span><span class=n>save</span><span class=p>()</span>
</span></span></code></pre></div><p>This approach takes 40 minutes to load all 50 million rows into Postgres, which is a significant improvement. This approach has its own challenges though, for example running it more than once will create duplicates in the destination table.</p><p>To keep the pipeline idempotent (no duplicates if we run it multiple times), we can load the data into a temporary table in Postgres. We then swap it with the production table inside a single transaction. Swapping avoids locking or partial state in case of failure, ensuring zero-downtime for consumers.</p><p>We can write this logic in the job itself or define it externally as separate steps to run before and after the job. Here’s what it looks like:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>writeToPostgresAppend</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>batch_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;jdbc:postgresql://localhost:5432/mydatabase&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>properties</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;user&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;password&#34;</span><span class=p>:</span> <span class=s2>&#34;password&#34;</span><span class=p>,</span> <span class=s2>&#34;driver&#34;</span><span class=p>:</span> <span class=s2>&#34;org.postgresql.Driver&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>jdbc</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>url</span><span class=p>,</span> <span class=n>table</span><span class=o>=</span><span class=s2>&#34;myschema.mytable_tmp&#34;</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;append&#34;</span><span class=p>,</span> <span class=n>properties</span><span class=o>=</span><span class=n>properties</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl><span class=c1># Create tmp table</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>psycopg</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=o>...</span><span class=p>)</span> <span class=k>as</span> <span class=n>conn</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>conn</span><span class=o>.</span><span class=n>transaction</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span> <span class=o>=</span> <span class=n>conn</span><span class=o>.</span><span class=n>cursor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s2>&#34;DROP TABLE IF EXISTS myschema.mytable_tmp;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s2>&#34;CREATE TABLE myschema.mytable_tmp (LIKE myschema.mytable INCLUDING ALL);&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run full load</span>
</span></span><span class=line><span class=cl><span class=n>stream</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;delta&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;checkpointLocation&#34;</span><span class=p>,</span> <span class=s2>&#34;/mnt/checkpoints/mycheckpoint&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>table</span><span class=p>(</span><span class=s2>&#34;public.mycatalog.mytable&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>trigger</span><span class=p>(</span><span class=n>availableNow</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>stream</span><span class=o>.</span><span class=n>writeStream</span><span class=o>.</span><span class=n>foreachBatch</span><span class=p>(</span><span class=n>writeToPostgresAppend</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>awaitTermination</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Swap tables</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>psycopg</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=o>...</span><span class=p>)</span> <span class=k>as</span> <span class=n>conn</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>conn</span><span class=o>.</span><span class=n>transaction</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span> <span class=o>=</span> <span class=n>conn</span><span class=o>.</span><span class=n>cursor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s2>&#34;ALTER TABLE myschema.mytable RENAME TO mytable_old;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s2>&#34;ALTER TABLE myschema.mytable_tmp RENAME TO mytable;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s2>&#34;DROP TABLE myschema.mytable_old;&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Now we can run this script to load all the data for the first time. If we run this multiple times it will safely swap tables without creating any downtime for our end-users consuming the data.</p><p>Note that we are not using Change Data Feed here because we want to move all the data across not the row-level changes. It is also important that you use a fresh checkpoint when you run this so that it loads all the data and not just the appends since the last execution.</p><h2 id=putting-it-all-together>Putting it all together<a hidden class=anchor aria-hidden=true href=#putting-it-all-together>#</a></h2><p>The final step to put these pipeline into production is the orchestration part. In an ideal world we would start the job when the source table updates. But for simplicity’s sake we can run it on a 5 minute schedule: if there’s been an update it will process it, if not it will quickly shut down.</p><p>To smoothly coordinate the transition from the initial load (using append) to ongoing upserts, the simplest approach is to define two separate jobs:</p><ul><li>One for the initial load, which we’ll run manually once.</li><li>One for the upserts, which we’ll scheduled to run regularly thereafter.</li></ul><p>It’s important that both jobs share the same checkpoint location so the upsert job can resume from where the initial load left off.</p><p>As for orchestrating tools, we can use the scheduling functionality from Databricks or an external tool like Airflow.</p><h2 id=serving-the-data>Serving the data<a hidden class=anchor aria-hidden=true href=#serving-the-data>#</a></h2><p>We now have the data landing in Postgres on a regular basis, so the last thing to do is to add indexes on the columns to improve reading speed. To know which indexes to create you need to look at your users’ query patterns.</p><p>Let’s say for example that most of the queries filter by either <code>user_id</code> or <code>age</code>. We could create the following indexes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=n>idx_user_id_btree</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>mytable</span><span class=w> </span><span class=p>(</span><span class=n>user_id</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=n>idx_age_btree</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>mytable</span><span class=w> </span><span class=p>(</span><span class=n>age</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>We’ve built a Reverse ETL pipeline that upserts data from Delta into Postgres using Structured Streaming. It runs every 30 minutes, handles updates with Change Data Feed, and stays idempotent to avoid duplication.</p><p>By combining batch-style streaming with deduplication and merge logic, we’ve made warehouse data available in Postgres. This data can now be read by downstream systems with sub-second latency.</p><p>Thank you for making it this far! I hope you’ve found the post interesting. Feel free to reach out to me on Linkedin or Substack!</p><p>Thanks to Oleksandr for providing feedback on earlier versions of this post.</p><hr><h2 id=my-newsletter>My Newsletter<a hidden class=anchor aria-hidden=true href=#my-newsletter>#</a></h2><p>I send out an email every so often about cool stuff I&rsquo;m building or working on.
<script src=https://f.convertkit.com/ckjs/ck.5.js></script><form action=https://app.kit.com/forms/8028704/subscriptions class="seva-form formkit-form" method=post data-sv-form=8028704 data-uid=b08f9955a0 data-format=inline data-version=5 data-options='{"settings":{"after_subscribe":{"action":"message","success_message":"Success! Now check your email to confirm your subscription.","redirect_url":""},"analytics":{"google":null,"fathom":null,"facebook":null,"segment":null,"pinterest":null,"sparkloop":null,"googletagmanager":null},"modal":{"trigger":"timer","scroll_percentage":null,"timer":5,"devices":"all","show_once_every":15},"powered_by":{"show":true,"url":"https://kit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic"},"recaptcha":{"enabled":false},"return_visitor":{"action":"show","custom_content":""},"slide_in":{"display_in":"bottom_right","trigger":"timer","scroll_percentage":null,"timer":5,"devices":"all","show_once_every":15},"sticky_bar":{"display_in":"top","trigger":"timer","scroll_percentage":null,"timer":5,"devices":"all","show_once_every":15}},"version":"5"}' min-width="400 500 600 700 800"><div data-style=clean><ul class="formkit-alert formkit-alert-error" data-element=errors data-group=alert></ul><div data-element=fields data-stacked=false class="seva-fields formkit-fields"><div class=formkit-field><input class=formkit-input name=email_address aria-label="Email Address" placeholder="Email Address" required type=email style=color:#000;border-color:#e3e3e3;border-radius:4px;font-weight:400></div><button data-element=submit class="formkit-submit formkit-submit" style=color:#fff;background-color:#1677be;border-radius:4px;font-weight:400><div class=formkit-spinner><div></div><div></div><div></div></div><span>Subscribe</span></button></div><div class=formkit-powered-by-convertkit-container><a href="https://kit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic" data-element=powered-by class=formkit-powered-by-convertkit data-variant=dark target=_blank rel=nofollow>Built with Kit</a></div></div><style>.formkit-form[data-uid=b08f9955a0] *{box-sizing:border-box}.formkit-form[data-uid=b08f9955a0]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.formkit-form[data-uid=b08f9955a0] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table}.formkit-form[data-uid=b08f9955a0] fieldset{border:0;padding:.01em 0 0;margin:0;min-width:0}.formkit-form[data-uid=b08f9955a0] body:not(:-moz-handler-blocked) fieldset{display:table-cell}.formkit-form[data-uid=b08f9955a0] h1,.formkit-form[data-uid=b08f9955a0] h2,.formkit-form[data-uid=b08f9955a0] h3,.formkit-form[data-uid=b08f9955a0] h4,.formkit-form[data-uid=b08f9955a0] h5,.formkit-form[data-uid=b08f9955a0] h6{color:inherit;font-size:inherit;font-weight:inherit}.formkit-form[data-uid=b08f9955a0] h2{font-size:1.5em;margin:1em 0}.formkit-form[data-uid=b08f9955a0] h3{font-size:1.17em;margin:1em 0}.formkit-form[data-uid=b08f9955a0] p{color:inherit;font-size:inherit;font-weight:inherit}.formkit-form[data-uid=b08f9955a0] ol:not([template-default]),.formkit-form[data-uid=b08f9955a0] ul:not([template-default]),.formkit-form[data-uid=b08f9955a0] blockquote:not([template-default]){text-align:left}.formkit-form[data-uid=b08f9955a0] p:not([template-default]),.formkit-form[data-uid=b08f9955a0] hr:not([template-default]),.formkit-form[data-uid=b08f9955a0] blockquote:not([template-default]),.formkit-form[data-uid=b08f9955a0] ol:not([template-default]),.formkit-form[data-uid=b08f9955a0] ul:not([template-default]){color:inherit;font-style:initial}.formkit-form[data-uid=b08f9955a0] .ordered-list,.formkit-form[data-uid=b08f9955a0] .unordered-list{list-style-position:outside!important;padding-left:1em}.formkit-form[data-uid=b08f9955a0] .list-item{padding-left:0}.formkit-form[data-uid=b08f9955a0][data-format=modal]{display:none}.formkit-form[data-uid=b08f9955a0][data-format="slide in"]{display:none}.formkit-form[data-uid=b08f9955a0][data-format="sticky bar"]{display:none}.formkit-sticky-bar .formkit-form[data-uid=b08f9955a0][data-format="sticky bar"]{display:block}.formkit-form[data-uid=b08f9955a0] .formkit-input,.formkit-form[data-uid=b08f9955a0] .formkit-select,.formkit-form[data-uid=b08f9955a0] .formkit-checkboxes{width:100%}.formkit-form[data-uid=b08f9955a0] .formkit-button,.formkit-form[data-uid=b08f9955a0] .formkit-submit{border:0;border-radius:5px;color:#fff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle}.formkit-form[data-uid=b08f9955a0] .formkit-button:hover,.formkit-form[data-uid=b08f9955a0] .formkit-submit:hover,.formkit-form[data-uid=b08f9955a0] .formkit-button:focus,.formkit-form[data-uid=b08f9955a0] .formkit-submit:focus{outline:none}.formkit-form[data-uid=b08f9955a0] .formkit-button:hover>span,.formkit-form[data-uid=b08f9955a0] .formkit-submit:hover>span,.formkit-form[data-uid=b08f9955a0] .formkit-button:focus>span,.formkit-form[data-uid=b08f9955a0] .formkit-submit:focus>span{background-color:rgba(0,0,0,.1)}.formkit-form[data-uid=b08f9955a0] .formkit-button>span,.formkit-form[data-uid=b08f9955a0] .formkit-submit>span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px}.formkit-form[data-uid=b08f9955a0] .formkit-input{background:#fff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms}.formkit-form[data-uid=b08f9955a0] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms}.formkit-form[data-uid=b08f9955a0] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:.8}.formkit-form[data-uid=b08f9955a0] .formkit-input::-moz-placeholder{color:inherit;opacity:.8}.formkit-form[data-uid=b08f9955a0] .formkit-input:-ms-input-placeholder{color:inherit;opacity:.8}.formkit-form[data-uid=b08f9955a0] .formkit-input::placeholder{color:inherit;opacity:.8}.formkit-form[data-uid=b08f9955a0] [data-group=dropdown]{position:relative;display:inline-block;width:100%}.formkit-form[data-uid=b08f9955a0] [data-group=dropdown]::before{content:"";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0;height:0;width:0;z-index:999}.formkit-form[data-uid=b08f9955a0] [data-group=dropdown] select{height:auto;width:100%;cursor:pointer;color:#333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#fff}.formkit-form[data-uid=b08f9955a0] [data-group=dropdown] select:focus{outline:none}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes]{text-align:left;margin:0}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox]{margin-bottom:10px}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] *{cursor:pointer}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox]:last-of-type{margin-bottom:0}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] input[type=checkbox]{display:none}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] input[type=checkbox]+label::after{content:none}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] input[type=checkbox]:checked+label::after{border-color:#fff;content:""}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] input[type=checkbox]:checked+label::before{background:#10bf7a;border-color:#10bf7a}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] label{position:relative;display:inline-block;padding-left:28px}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] label::before,.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] label::after{position:absolute;content:"";display:inline-block}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#fff;left:0;top:3px}.formkit-form[data-uid=b08f9955a0] [data-group=checkboxes] [data-group=checkbox] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px}.formkit-form[data-uid=b08f9955a0] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%}.formkit-form[data-uid=b08f9955a0] .formkit-alert:empty{display:none}.formkit-form[data-uid=b08f9955a0] .formkit-alert-success{background:#d3fbeb;border-color:#10bf7a;color:#0c905c}.formkit-form[data-uid=b08f9955a0] .formkit-alert-error{background:#fde8e2;border-color:#f2643b;color:#ea4110}.formkit-form[data-uid=b08f9955a0] .formkit-spinner{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:0;width:0;margin:0 auto;position:absolute;top:0;left:0;right:0;width:0;overflow:hidden;text-align:center;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out}.formkit-form[data-uid=b08f9955a0] .formkit-spinner>div{margin:auto;width:12px;height:12px;background-color:#fff;opacity:.3;border-radius:100%;display:inline-block;-webkit-animation:formkit-bouncedelay-formkit-form-data-uid-b08f9955a0- 1.4s infinite ease-in-out both;animation:formkit-bouncedelay-formkit-form-data-uid-b08f9955a0- 1.4s infinite ease-in-out both}.formkit-form[data-uid=b08f9955a0] .formkit-spinner>div:nth-child(1){-webkit-animation-delay:-.32s;animation-delay:-.32s}.formkit-form[data-uid=b08f9955a0] .formkit-spinner>div:nth-child(2){-webkit-animation-delay:-.16s;animation-delay:-.16s}.formkit-form[data-uid=b08f9955a0] .formkit-submit[data-active] .formkit-spinner{opacity:1;height:100%;width:50px}.formkit-form[data-uid=b08f9955a0] .formkit-submit[data-active] .formkit-spinner~span{opacity:0}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by[data-active=false]{opacity:.35}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;margin:10px 0;position:relative}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit-container[data-active=false]{opacity:.35}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff;border-radius:9px;color:#3d3d3d;cursor:pointer;display:block;height:36px;margin:0 auto;opacity:.95;padding:0;-webkit-text-decoration:none;text-decoration:none;text-indent:100%;-webkit-transition:ease-in-out all 200ms;transition:ease-in-out all 200ms;white-space:nowrap;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:157px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='133' height='36' viewBox='0 0 133 36' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M0.861 25.5C0.735 25.5 0.651 25.416 0.651 25.29V10.548C0.651 10.422 0.735 10.338 0.861 10.338H6.279C9.072 10.338 10.668 11.451 10.668 13.824C10.668 15.819 9.219 16.932 8.001 17.226C7.707 17.268 7.707 17.625 8.022 17.688C9.912 18.108 11.088 19.116 11.088 21.321C11.088 23.715 9.429 25.5 6.426 25.5H0.861ZM5.397 23.085C6.825 23.085 7.518 22.224 7.518 21.006C7.518 19.683 6.825 18.948 5.397 18.948H4.2V23.085H5.397ZM5.313 16.617C6.51 16.617 7.245 15.945 7.245 14.601C7.245 13.383 6.51 12.753 5.25 12.753H4.2V16.617H5.313ZM17.9758 23.883C17.9758 23.568 17.6608 23.505 17.5348 23.799C17.0308 24.954 16.1698 25.731 14.5528 25.731C12.8728 25.731 12.0958 24.471 12.0958 22.707V14.937C12.0958 14.811 12.1798 14.727 12.3058 14.727H15.2248C15.3508 14.727 15.4348 14.811 15.4348 14.937V21.657C15.4348 22.581 15.7708 23.022 16.4638 23.022C17.1778 23.022 17.6188 22.581 17.6188 21.657V14.937C17.6188 14.811 17.7028 14.727 17.8288 14.727H20.7478C20.8738 14.727 20.9578 14.811 20.9578 14.937V25.29C20.9578 25.416 20.8738 25.5 20.7478 25.5H18.1858C18.0598 25.5 17.9758 25.416 17.9758 25.29V23.883ZM25.6141 25.29C25.6141 25.416 25.5301 25.5 25.4041 25.5H22.4851C22.3591 25.5 22.2751 25.416 22.2751 25.29V14.937C22.2751 14.811 22.3591 14.727 22.4851 14.727H25.4041C25.5301 14.727 25.6141 14.811 25.6141 14.937V25.29ZM23.9131 13.74C22.8001 13.74 22.0441 12.942 22.0441 11.934C22.0441 10.926 22.8001 10.107 23.9131 10.107C25.0051 10.107 25.7611 10.926 25.7611 11.934C25.7611 12.942 25.0051 13.74 23.9131 13.74ZM26.7883 10.548C26.7883 10.422 26.8723 10.338 26.9983 10.338H29.9173C30.0433 10.338 30.1273 10.422 30.1273 10.548V22.056C30.1273 22.749 30.2533 23.085 30.8203 23.085C31.0093 23.085 31.1983 23.043 31.3663 23.001C31.5133 22.959 31.6183 22.959 31.6183 23.127V25.059C31.6183 25.164 31.5763 25.269 31.4923 25.311C30.9673 25.521 30.2953 25.71 29.5813 25.71C27.7123 25.71 26.7883 24.639 26.7883 22.476V10.548ZM32.4237 14.727C32.8227 14.727 32.9277 14.538 32.9697 14.055L33.1167 12.039C33.1167 11.913 33.2217 11.829 33.3477 11.829H35.8887C36.0147 11.829 36.0987 11.913 36.0987 12.039V14.517C36.0987 14.643 36.1827 14.727 36.3087 14.727H38.2827C38.4087 14.727 38.4927 14.811 38.4927 14.937V16.659C38.4927 16.785 38.4087 16.869 38.2827 16.869H36.0777V22.056C36.0777 22.875 36.5397 23.085 37.0647 23.085C37.4847 23.085 37.9467 22.938 38.3247 22.707C38.4717 22.623 38.5767 22.665 38.5767 22.833V24.828C38.5767 24.933 38.5347 25.017 38.4507 25.08C37.8417 25.458 36.9807 25.71 36.0357 25.71C34.2927 25.71 32.7387 24.912 32.7387 22.476V16.869H31.8567C31.7307 16.869 31.6467 16.785 31.6467 16.659V14.937C31.6467 14.811 31.7307 14.727 31.8567 14.727H32.4237ZM51.3808 14.727C51.5068 14.727 51.5908 14.79 51.6118 14.916L52.3888 19.851L52.5778 21.174C52.6198 21.468 52.9558 21.468 52.9768 21.174C53.0398 20.712 53.0818 20.271 53.1658 19.83L53.8798 14.916C53.9008 14.79 53.9848 14.727 54.1108 14.727H56.6728C56.8198 14.727 56.8828 14.811 56.8618 14.958L54.6778 25.311C54.6568 25.437 54.5728 25.5 54.4468 25.5H51.3178C51.1918 25.5 51.1078 25.437 51.0868 25.311L50.1208 20.082L49.8898 18.633C49.8688 18.444 49.6588 18.444 49.6378 18.633L49.4068 20.103L48.5458 25.311C48.5248 25.437 48.4408 25.5 48.3148 25.5H45.2068C45.0808 25.5 44.9968 25.437 44.9758 25.311L42.8128 14.958C42.7918 14.811 42.8548 14.727 43.0018 14.727H45.9628C46.0888 14.727 46.1728 14.79 46.1938 14.916L46.9288 19.83C47.0128 20.271 47.0758 20.754 47.1388 21.195C47.2018 21.51 47.4748 21.531 47.5378 21.195L47.7478 19.872L48.6088 14.916C48.6298 14.79 48.7138 14.727 48.8398 14.727H51.3808ZM61.1582 25.29C61.1582 25.416 61.0742 25.5 60.9482 25.5H58.0292C57.9032 25.5 57.8192 25.416 57.8192 25.29V14.937C57.8192 14.811 57.9032 14.727 58.0292 14.727H60.9482C61.0742 14.727 61.1582 14.811 61.1582 14.937V25.29ZM59.4572 13.74C58.3442 13.74 57.5882 12.942 57.5882 11.934C57.5882 10.926 58.3442 10.107 59.4572 10.107C60.5492 10.107 61.3052 10.926 61.3052 11.934C61.3052 12.942 60.5492 13.74 59.4572 13.74ZM62.8154 14.727C63.2144 14.727 63.3194 14.538 63.3614 14.055L63.5084 12.039C63.5084 11.913 63.6134 11.829 63.7394 11.829H66.2804C66.4064 11.829 66.4904 11.913 66.4904 12.039V14.517C66.4904 14.643 66.5744 14.727 66.7004 14.727H68.6744C68.8004 14.727 68.8844 14.811 68.8844 14.937V16.659C68.8844 16.785 68.8004 16.869 68.6744 16.869H66.4694V22.056C66.4694 22.875 66.9314 23.085 67.4564 23.085C67.8764 23.085 68.3384 22.938 68.7164 22.707C68.8634 22.623 68.9684 22.665 68.9684 22.833V24.828C68.9684 24.933 68.9264 25.017 68.8424 25.08C68.2334 25.458 67.3724 25.71 66.4274 25.71C64.6844 25.71 63.1304 24.912 63.1304 22.476V16.869H62.2484C62.1224 16.869 62.0384 16.785 62.0384 16.659V14.937C62.0384 14.811 62.1224 14.727 62.2484 14.727H62.8154ZM73.4298 16.323C73.4298 16.638 73.7868 16.68 73.9128 16.407C74.3748 15.315 75.1308 14.496 76.6008 14.496C78.2178 14.496 78.9528 15.609 78.9528 17.373V25.29C78.9528 25.416 78.8688 25.5 78.7428 25.5H75.8238C75.6978 25.5 75.6138 25.416 75.6138 25.29V18.633C75.6138 17.709 75.2778 17.268 74.5848 17.268C73.8708 17.268 73.4298 17.709 73.4298 18.633V25.29C73.4298 25.416 73.3458 25.5 73.2198 25.5H70.3008C70.1748 25.5 70.0908 25.416 70.0908 25.29V10.548C70.0908 10.422 70.1748 10.338 70.3008 10.338H73.2198C73.3458 10.338 73.4298 10.422 73.4298 10.548V16.323Z' fill='%231E1E1E'/%3E%3Cpath d='M100.132 16.3203C105.58 17.3761 107.272 22.4211 107.318 27.4961C107.318 27.6101 107.226 27.7041 107.112 27.7041H100.252C100.138 27.7041 100.046 27.6121 100.046 27.5001C100.026 23.5629 99.3877 20.0896 95.4865 19.9396C95.3705 19.9356 95.2725 20.0276 95.2725 20.1456V27.5001C95.2725 27.6141 95.1806 27.7061 95.0666 27.7061H88.206C88.092 27.7061 88 27.6141 88 27.5001V8.75585C88 8.64187 88.092 8.54989 88.206 8.54989H95.0686C95.1826 8.54989 95.2745 8.64187 95.2745 8.75585V15.7764C95.2745 15.8804 95.3585 15.9644 95.4625 15.9644C95.5445 15.9644 95.6185 15.9104 95.6425 15.8324C97.4081 10.0416 100.709 8.58588 106.07 8.55189C106.184 8.55189 106.276 8.64387 106.276 8.75785V15.7604C106.276 15.8744 106.184 15.9664 106.07 15.9664H100.166C100.066 15.9664 99.9856 16.0464 99.9856 16.1464C99.9856 16.2304 100.048 16.3043 100.132 16.3203ZM118.918 20.7095V16.1704C118.918 16.0564 119.01 15.9644 119.124 15.9644H124.173C124.273 15.9644 124.353 15.8844 124.353 15.7844C124.353 15.6985 124.291 15.6245 124.207 15.6085C120.256 14.8246 118.432 12.5511 118.37 8.75585C118.368 8.64387 118.458 8.54989 118.572 8.54989H125.986C126.1 8.54989 126.192 8.64187 126.192 8.75585V11.9532C126.192 12.0672 126.284 12.1592 126.398 12.1592H130.649C130.763 12.1592 130.855 12.2511 130.855 12.3651V15.7624C130.855 15.8764 130.763 15.9684 130.649 15.9684H126.398C126.284 15.9684 126.192 16.0604 126.192 16.1744V19.8356C126.192 21.1294 126.986 21.5553 128.04 21.5553C129.692 21.5553 131.323 20.8114 131.977 20.4735C132.113 20.4035 132.277 20.5015 132.277 20.6555V26.3543C132.277 26.5063 132.193 26.6463 132.059 26.7183C131.413 27.0582 129.418 28 127.136 28C122.435 27.996 118.918 26.0824 118.918 20.7095ZM109.266 27.4981V16.1704C109.266 16.0564 109.358 15.9644 109.472 15.9644H116.334C116.448 15.9644 116.54 16.0564 116.54 16.1704V27.4981C116.54 27.6121 116.448 27.7041 116.334 27.7041H109.472C109.358 27.7021 109.266 27.6101 109.266 27.4981ZM108.876 11.4913C108.876 13.4189 110.238 14.9826 112.853 14.9826C115.469 14.9826 116.83 13.4189 116.83 11.4913C116.83 9.56369 115.471 8 112.853 8C110.238 8 108.876 9.56369 108.876 11.4913Z' fill='%231E1E1E'/%3E%3C/svg%3E")}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit:hover,.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit:focus{background-color:#fff;-webkit-transform:scale(1.025)perspective(1px);-ms-transform:scale(1.025)perspective(1px);transform:scale(1.025)perspective(1px);opacity:1}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit[data-variant=dark],.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit[data-variant=light]{background-color:transparent;border-color:transparent;width:133px}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit[data-variant=light]{color:#fff;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='133' height='36' viewBox='0 0 133 36' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M0.861 25.5C0.735 25.5 0.651 25.416 0.651 25.29V10.548C0.651 10.422 0.735 10.338 0.861 10.338H6.279C9.072 10.338 10.668 11.451 10.668 13.824C10.668 15.819 9.219 16.932 8.001 17.226C7.707 17.268 7.707 17.625 8.022 17.688C9.912 18.108 11.088 19.116 11.088 21.321C11.088 23.715 9.429 25.5 6.426 25.5H0.861ZM5.397 23.085C6.825 23.085 7.518 22.224 7.518 21.006C7.518 19.683 6.825 18.948 5.397 18.948H4.2V23.085H5.397ZM5.313 16.617C6.51 16.617 7.245 15.945 7.245 14.601C7.245 13.383 6.51 12.753 5.25 12.753H4.2V16.617H5.313ZM17.9758 23.883C17.9758 23.568 17.6608 23.505 17.5348 23.799C17.0308 24.954 16.1698 25.731 14.5528 25.731C12.8728 25.731 12.0958 24.471 12.0958 22.707V14.937C12.0958 14.811 12.1798 14.727 12.3058 14.727H15.2248C15.3508 14.727 15.4348 14.811 15.4348 14.937V21.657C15.4348 22.581 15.7708 23.022 16.4638 23.022C17.1778 23.022 17.6188 22.581 17.6188 21.657V14.937C17.6188 14.811 17.7028 14.727 17.8288 14.727H20.7478C20.8738 14.727 20.9578 14.811 20.9578 14.937V25.29C20.9578 25.416 20.8738 25.5 20.7478 25.5H18.1858C18.0598 25.5 17.9758 25.416 17.9758 25.29V23.883ZM25.6141 25.29C25.6141 25.416 25.5301 25.5 25.4041 25.5H22.4851C22.3591 25.5 22.2751 25.416 22.2751 25.29V14.937C22.2751 14.811 22.3591 14.727 22.4851 14.727H25.4041C25.5301 14.727 25.6141 14.811 25.6141 14.937V25.29ZM23.9131 13.74C22.8001 13.74 22.0441 12.942 22.0441 11.934C22.0441 10.926 22.8001 10.107 23.9131 10.107C25.0051 10.107 25.7611 10.926 25.7611 11.934C25.7611 12.942 25.0051 13.74 23.9131 13.74ZM26.7883 10.548C26.7883 10.422 26.8723 10.338 26.9983 10.338H29.9173C30.0433 10.338 30.1273 10.422 30.1273 10.548V22.056C30.1273 22.749 30.2533 23.085 30.8203 23.085C31.0093 23.085 31.1983 23.043 31.3663 23.001C31.5133 22.959 31.6183 22.959 31.6183 23.127V25.059C31.6183 25.164 31.5763 25.269 31.4923 25.311C30.9673 25.521 30.2953 25.71 29.5813 25.71C27.7123 25.71 26.7883 24.639 26.7883 22.476V10.548ZM32.4237 14.727C32.8227 14.727 32.9277 14.538 32.9697 14.055L33.1167 12.039C33.1167 11.913 33.2217 11.829 33.3477 11.829H35.8887C36.0147 11.829 36.0987 11.913 36.0987 12.039V14.517C36.0987 14.643 36.1827 14.727 36.3087 14.727H38.2827C38.4087 14.727 38.4927 14.811 38.4927 14.937V16.659C38.4927 16.785 38.4087 16.869 38.2827 16.869H36.0777V22.056C36.0777 22.875 36.5397 23.085 37.0647 23.085C37.4847 23.085 37.9467 22.938 38.3247 22.707C38.4717 22.623 38.5767 22.665 38.5767 22.833V24.828C38.5767 24.933 38.5347 25.017 38.4507 25.08C37.8417 25.458 36.9807 25.71 36.0357 25.71C34.2927 25.71 32.7387 24.912 32.7387 22.476V16.869H31.8567C31.7307 16.869 31.6467 16.785 31.6467 16.659V14.937C31.6467 14.811 31.7307 14.727 31.8567 14.727H32.4237ZM51.3808 14.727C51.5068 14.727 51.5908 14.79 51.6118 14.916L52.3888 19.851L52.5778 21.174C52.6198 21.468 52.9558 21.468 52.9768 21.174C53.0398 20.712 53.0818 20.271 53.1658 19.83L53.8798 14.916C53.9008 14.79 53.9848 14.727 54.1108 14.727H56.6728C56.8198 14.727 56.8828 14.811 56.8618 14.958L54.6778 25.311C54.6568 25.437 54.5728 25.5 54.4468 25.5H51.3178C51.1918 25.5 51.1078 25.437 51.0868 25.311L50.1208 20.082L49.8898 18.633C49.8688 18.444 49.6588 18.444 49.6378 18.633L49.4068 20.103L48.5458 25.311C48.5248 25.437 48.4408 25.5 48.3148 25.5H45.2068C45.0808 25.5 44.9968 25.437 44.9758 25.311L42.8128 14.958C42.7918 14.811 42.8548 14.727 43.0018 14.727H45.9628C46.0888 14.727 46.1728 14.79 46.1938 14.916L46.9288 19.83C47.0128 20.271 47.0758 20.754 47.1388 21.195C47.2018 21.51 47.4748 21.531 47.5378 21.195L47.7478 19.872L48.6088 14.916C48.6298 14.79 48.7138 14.727 48.8398 14.727H51.3808ZM61.1582 25.29C61.1582 25.416 61.0742 25.5 60.9482 25.5H58.0292C57.9032 25.5 57.8192 25.416 57.8192 25.29V14.937C57.8192 14.811 57.9032 14.727 58.0292 14.727H60.9482C61.0742 14.727 61.1582 14.811 61.1582 14.937V25.29ZM59.4572 13.74C58.3442 13.74 57.5882 12.942 57.5882 11.934C57.5882 10.926 58.3442 10.107 59.4572 10.107C60.5492 10.107 61.3052 10.926 61.3052 11.934C61.3052 12.942 60.5492 13.74 59.4572 13.74ZM62.8154 14.727C63.2144 14.727 63.3194 14.538 63.3614 14.055L63.5084 12.039C63.5084 11.913 63.6134 11.829 63.7394 11.829H66.2804C66.4064 11.829 66.4904 11.913 66.4904 12.039V14.517C66.4904 14.643 66.5744 14.727 66.7004 14.727H68.6744C68.8004 14.727 68.8844 14.811 68.8844 14.937V16.659C68.8844 16.785 68.8004 16.869 68.6744 16.869H66.4694V22.056C66.4694 22.875 66.9314 23.085 67.4564 23.085C67.8764 23.085 68.3384 22.938 68.7164 22.707C68.8634 22.623 68.9684 22.665 68.9684 22.833V24.828C68.9684 24.933 68.9264 25.017 68.8424 25.08C68.2334 25.458 67.3724 25.71 66.4274 25.71C64.6844 25.71 63.1304 24.912 63.1304 22.476V16.869H62.2484C62.1224 16.869 62.0384 16.785 62.0384 16.659V14.937C62.0384 14.811 62.1224 14.727 62.2484 14.727H62.8154ZM73.4298 16.323C73.4298 16.638 73.7868 16.68 73.9128 16.407C74.3748 15.315 75.1308 14.496 76.6008 14.496C78.2178 14.496 78.9528 15.609 78.9528 17.373V25.29C78.9528 25.416 78.8688 25.5 78.7428 25.5H75.8238C75.6978 25.5 75.6138 25.416 75.6138 25.29V18.633C75.6138 17.709 75.2778 17.268 74.5848 17.268C73.8708 17.268 73.4298 17.709 73.4298 18.633V25.29C73.4298 25.416 73.3458 25.5 73.2198 25.5H70.3008C70.1748 25.5 70.0908 25.416 70.0908 25.29V10.548C70.0908 10.422 70.1748 10.338 70.3008 10.338H73.2198C73.3458 10.338 73.4298 10.422 73.4298 10.548V16.323Z' fill='white'/%3E%3Cpath d='M100.132 16.3203C105.58 17.3761 107.272 22.4211 107.318 27.4961C107.318 27.6101 107.226 27.7041 107.112 27.7041H100.252C100.138 27.7041 100.046 27.6121 100.046 27.5001C100.026 23.5629 99.3877 20.0896 95.4865 19.9396C95.3705 19.9356 95.2725 20.0276 95.2725 20.1456V27.5001C95.2725 27.6141 95.1806 27.7061 95.0666 27.7061H88.206C88.092 27.7061 88 27.6141 88 27.5001V8.75585C88 8.64187 88.092 8.54989 88.206 8.54989H95.0686C95.1826 8.54989 95.2745 8.64187 95.2745 8.75585V15.7764C95.2745 15.8804 95.3585 15.9644 95.4625 15.9644C95.5445 15.9644 95.6185 15.9104 95.6425 15.8324C97.4081 10.0416 100.709 8.58588 106.07 8.55189C106.184 8.55189 106.276 8.64387 106.276 8.75785V15.7604C106.276 15.8744 106.184 15.9664 106.07 15.9664H100.166C100.066 15.9664 99.9856 16.0464 99.9856 16.1464C99.9856 16.2304 100.048 16.3043 100.132 16.3203ZM118.918 20.7095V16.1704C118.918 16.0564 119.01 15.9644 119.124 15.9644H124.173C124.273 15.9644 124.353 15.8844 124.353 15.7844C124.353 15.6985 124.291 15.6245 124.207 15.6085C120.256 14.8246 118.432 12.5511 118.37 8.75585C118.368 8.64387 118.458 8.54989 118.572 8.54989H125.986C126.1 8.54989 126.192 8.64187 126.192 8.75585V11.9532C126.192 12.0672 126.284 12.1592 126.398 12.1592H130.649C130.763 12.1592 130.855 12.2511 130.855 12.3651V15.7624C130.855 15.8764 130.763 15.9684 130.649 15.9684H126.398C126.284 15.9684 126.192 16.0604 126.192 16.1744V19.8356C126.192 21.1294 126.986 21.5553 128.04 21.5553C129.692 21.5553 131.323 20.8114 131.977 20.4735C132.113 20.4035 132.277 20.5015 132.277 20.6555V26.3543C132.277 26.5063 132.193 26.6463 132.059 26.7183C131.413 27.0582 129.418 28 127.136 28C122.435 27.996 118.918 26.0824 118.918 20.7095ZM109.266 27.4981V16.1704C109.266 16.0564 109.358 15.9644 109.472 15.9644H116.334C116.448 15.9644 116.54 16.0564 116.54 16.1704V27.4981C116.54 27.6121 116.448 27.7041 116.334 27.7041H109.472C109.358 27.7021 109.266 27.6101 109.266 27.4981ZM108.876 11.4913C108.876 13.4189 110.238 14.9826 112.853 14.9826C115.469 14.9826 116.83 13.4189 116.83 11.4913C116.83 9.56369 115.471 8 112.853 8C110.238 8 108.876 9.56369 108.876 11.4913Z' fill='white'/%3E%3C/svg%3E")}@-webkit-keyframes formkit-bouncedelay-formkit-form-data-uid-b08f9955a0-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0)}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}@keyframes formkit-bouncedelay-formkit-form-data-uid-b08f9955a0-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0)}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1)}}.formkit-form[data-uid=b08f9955a0] blockquote{padding:10px 20px;margin:0 0 20px;border-left:5px solid #e1e1e1}.formkit-form[data-uid=b08f9955a0] .seva-custom-content{padding:15px;font-size:16px;color:#fff;mix-blend-mode:difference}.formkit-form[data-uid=b08f9955a0] .formkit-modal.guard{max-width:420px;width:100%}.formkit-form[data-uid=b08f9955a0]{max-width:700px}.formkit-form[data-uid=b08f9955a0] [data-style=clean]{width:100%}.formkit-form[data-uid=b08f9955a0] .formkit-fields{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:0 auto}.formkit-form[data-uid=b08f9955a0] .formkit-field,.formkit-form[data-uid=b08f9955a0] .formkit-submit{margin:0 0 15px;-webkit-flex:1 0 100%;-ms-flex:1 0 100%;flex:1 0 100%}.formkit-form[data-uid=b08f9955a0] .formkit-powered-by-convertkit-container{margin:0}.formkit-form[data-uid=b08f9955a0] .formkit-submit{position:static}.formkit-form[data-uid=b08f9955a0][min-width~="700"] [data-style=clean],.formkit-form[data-uid=b08f9955a0][min-width~="800"] [data-style=clean]{padding:10px;padding-top:56px}.formkit-form[data-uid=b08f9955a0][min-width~="700"] .formkit-fields[data-stacked=false],.formkit-form[data-uid=b08f9955a0][min-width~="800"] .formkit-fields[data-stacked=false]{margin-left:-5px;margin-right:-5px}.formkit-form[data-uid=b08f9955a0][min-width~="700"] .formkit-fields[data-stacked=false] .formkit-field,.formkit-form[data-uid=b08f9955a0][min-width~="800"] .formkit-fields[data-stacked=false] .formkit-field,.formkit-form[data-uid=b08f9955a0][min-width~="700"] .formkit-fields[data-stacked=false] .formkit-submit,.formkit-form[data-uid=b08f9955a0][min-width~="800"] .formkit-fields[data-stacked=false] .formkit-submit{margin:0 5px 15px}.formkit-form[data-uid=b08f9955a0][min-width~="700"] .formkit-fields[data-stacked=false] .formkit-field,.formkit-form[data-uid=b08f9955a0][min-width~="800"] .formkit-fields[data-stacked=false] .formkit-field{-webkit-flex:100 1 auto;-ms-flex:100 1 auto;flex:100 1 auto}.formkit-form[data-uid=b08f9955a0][min-width~="700"] .formkit-fields[data-stacked=false] .formkit-submit,.formkit-form[data-uid=b08f9955a0][min-width~="800"] .formkit-fields[data-stacked=false] .formkit-submit{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:auto}</style></form></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/postgres/>Postgres</a></li><li><a href=http://localhost:1313/tags/databricks/>Databricks</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/postgres_replication/><span class=title>« Prev</span><br><span>Postgres Replication</span>
</a><a class=next href=http://localhost:1313/posts/postgres_bad_partitions/><span class=title>Next »</span><br><span>We Need to Talk about Partitions</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on x" href="https://x.com/intent/tweet/?text=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f&amp;hashtags=postgres%2cdatabricks"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f&amp;title=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming&amp;summary=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f&title=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on whatsapp" href="https://api.whatsapp.com/send?text=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on telegram" href="https://telegram.me/share/url?text=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20a%20Reverse%20ETL%20Pipeline%3a%20Upserting%20Delta%20Lake%20Data%20into%20Postgres%20with%20Structured%20Streaming&u=http%3a%2f%2flocalhost%3a1313%2fposts%2freverse_etl_databricks_to_postgres%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Pablo Lopez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>