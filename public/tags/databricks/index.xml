<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Databricks on Pablo Lopez</title>
    <link>http://localhost:1313/tags/databricks/</link>
    <description>Recent content in Databricks on Pablo Lopez</description>
    <image>
      <title>Pablo Lopez</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.3</generator>
    <language>en</language>
    <lastBuildDate>Thu, 08 May 2025 13:21:30 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/databricks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building a Reverse ETL Pipeline: Upserting Delta Lake Data into Postgres with Structured Streaming</title>
      <link>http://localhost:1313/posts/reverse_etl_databricks_to_postgres/</link>
      <pubDate>Thu, 08 May 2025 13:21:30 +0100</pubDate>
      <guid>http://localhost:1313/posts/reverse_etl_databricks_to_postgres/</guid>
      <description>&lt;p&gt;In this post, I share how to build a Reverse ETL pipeline to upsert data from Delta (Databricks) into Postgres to provide sub-second response times to our tables.&lt;/p&gt;
&lt;p&gt;The goal is to make warehouse data available to downstream systems that require millisecond response times. These systems could be front-end applications that need to consume this data, or online machine learning models that require additional data as input to generate predictions in real time.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
